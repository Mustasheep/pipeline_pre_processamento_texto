{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORj+B/3PcFsgtjBBoS8/33"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "Durante minhas aulas de processamento de linguagem natural com a IA (sim, a IA é minha professora), alcançamos a primeira atividade prática e irei construir um pipeline para meus futuros trabalhos de PLN.\n",
        "\n",
        "Eu estou ainda no Nível 1 do curso de PLN e estamos encerrando a primeira semana de estudos. Este nível tem duração de 30 dias (definido pela professora) e está sendo um aprendizado enriquecedor.\n",
        "\n",
        "Meu objetivo inicial é criar as Funções de pré-processamento:\n",
        "\n",
        "- `remover_ruido(texto)`\n",
        "- `tokenizar_palavras(texto)`\n",
        "- `tokenizar_sentencas(texto)`\n",
        "- `remover_stop_words(tokens)`\n",
        "- `aplicar_stemming(tokens)`\n",
        "- `aplicar_lematizacao(tokens)`"
      ],
      "metadata": {
        "id": "tsZqZ8xiWiav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando bibliotecas\n",
        "\n",
        "Irei carregarregar as bibliotecas `re` para fazer a remoção de ruídos, alguns pacotes do `nltk` que possui grande utilidade para pré-processamento e para o PLN em geral. E por fim, o `spacy` e o `unidecode` por terem suporte para palavras da língua portuguesa para concluir algumas etapas."
      ],
      "metadata": {
        "id": "o4sf37BWYBEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "import spacy\n",
        "\n",
        "# Baixando dependências\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvNPef37Qpvj",
        "outputId": "1203be1c-128d-4d8e-e61e-09059c274da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando função para cada etapa de pré-processamento\n",
        "As funções a seguir serão para remover o ruído (caracteres especiais, URLs, espaços em branco, números, tags HTML, acentuação e padronizar em letras minúsculas). Faremos também a tokenização das palavras e sentenças, remoção dos stop words (preposições, artigos, conjunções, etc), stemming e lematização."
      ],
      "metadata": {
        "id": "MTEiXXeMua-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remover_ruido(texto):\n",
        "  texto = re.sub(r'<.*?>', '', texto) # Remove tags HTML\n",
        "  texto = unidecode(texto) # Remove acentuação preservando a letra\n",
        "  texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto) # Remove caracteres especiais\n",
        "  texto = re.sub(r'http\\S+', '', texto) # Remove URLs\n",
        "  texto = re.sub(r'\\s+', ' ', texto) # Remove espaços em branco extras\n",
        "  texto = texto.strip() # Remove espaços em branco no início e no final\n",
        "  texto = texto.lower() # Converte para minúsculas\n",
        "  texto = re.sub(r'\\d+', '', texto) # Remove números\n",
        "  return texto\n",
        "\n",
        "def tokeniza_palavras(texto):\n",
        "  tokens = word_tokenize(texto)\n",
        "  return tokens\n",
        "\n",
        "def tokeniza_sentencas(texto):\n",
        "  sentencas = sent_tokenize(texto)\n",
        "  return sentencas\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "  stop_words = set(stopwords.words('portuguese'))\n",
        "  tokens_s_sw = [token for token in tokens if token not in stop_words]\n",
        "  return tokens_s_sw\n",
        "\n",
        "def aplica_stemming(tokens_s_sw):\n",
        "  stemmer = RSLPStemmer()\n",
        "  tokens_stemmed = [stemmer.stem(token) for token in tokens_s_sw]\n",
        "  return tokens_stemmed\n",
        "\n",
        "def aplica_lematizacao(tokens_s_sw):\n",
        "  nlp = spacy.load('pt_core_news_sm')\n",
        "  tokens_lematizados = [token.lemma_ for token in nlp(' '.join(tokens_s_sw))]\n",
        "  return tokens_lematizados"
      ],
      "metadata": {
        "id": "BpioeyX7RooE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicando as funções em uma lista de textos"
      ],
      "metadata": {
        "id": "MbUlfooWDH_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Textos a serem trabalhados\n",
        "\n",
        "textos = [\n",
        "    \"<p>Este é um exemplo de texto <strong>com HTML</strong> e caracteres especiais! https://www.exemplo.com</p>\",\n",
        "    \"O GATO preto CORREU rapidamente e pulou a cerca.\",\n",
        "    \"A menina estava feliz, cantando e dançando no jardim.\",\n",
        "    \"O filme foi incrível, mas achei um pouco longo. Será que vale a pena assistir de novo?\",\n",
        "    \"A tecnologia está avançando a cada dia e transformando nossas vidas.\",\n",
        "    \"Existem muitos desafios no mundo da ciência de dados, mas também muitas oportunidades.\",\n",
        "    \"É importante estudar com dedicação para atingir nossos objetivos.\",\n",
        "    \"O processo de aprendizado de PLN pode ser desafiador, porém gratificante.\",\n",
        "    \"As redes sociais são uma ferramenta poderosa para comunicação e informação.\",\n",
        "    \"Eu adoro pizza de pepperoni, mas também amo lasanha à bolonhesa.\"\n",
        "]"
      ],
      "metadata": {
        "id": "9dqi_MFTudFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo ruídos e padronizando com letras minúsculas\n",
        "\n",
        "textos_limpos = [remover_ruido(texto) for texto in textos]\n",
        "textos_limpos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I430ELKcLq4",
        "outputId": "889dc1a7-f3bf-4e19-bc6d-c5f7f2bc30be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['este e um exemplo de texto com html e caracteres especiais',\n",
              " 'o gato preto correu rapidamente e pulou a cerca',\n",
              " 'a menina estava feliz cantando e dancando no jardim',\n",
              " 'o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo',\n",
              " 'a tecnologia esta avancando a cada dia e transformando nossas vidas',\n",
              " 'existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades',\n",
              " 'e importante estudar com dedicacao para atingir nossos objetivos',\n",
              " 'o processo de aprendizado de pln pode ser desafiador porem gratificante',\n",
              " 'as redes sociais sao uma ferramenta poderosa para comunicacao e informacao',\n",
              " 'eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de palavras\n",
        "\n",
        "textos_tokenizados_palavra = [tokeniza_palavras(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_palavra:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SAUWO_u6c9gH",
        "outputId": "aed8bf96-0d3f-4b96-dbdc-48d03fe55459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este', 'e', 'um', 'exemplo', 'de', 'texto', 'com', 'html', 'e', 'caracteres', 'especiais']\n",
            "['o', 'gato', 'preto', 'correu', 'rapidamente', 'e', 'pulou', 'a', 'cerca']\n",
            "['a', 'menina', 'estava', 'feliz', 'cantando', 'e', 'dancando', 'no', 'jardim']\n",
            "['o', 'filme', 'foi', 'incrivel', 'mas', 'achei', 'um', 'pouco', 'longo', 'sera', 'que', 'vale', 'a', 'pena', 'assistir', 'de', 'novo']\n",
            "['a', 'tecnologia', 'esta', 'avancando', 'a', 'cada', 'dia', 'e', 'transformando', 'nossas', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'no', 'mundo', 'da', 'ciencia', 'de', 'dados', 'mas', 'tambem', 'muitas', 'oportunidades']\n",
            "['e', 'importante', 'estudar', 'com', 'dedicacao', 'para', 'atingir', 'nossos', 'objetivos']\n",
            "['o', 'processo', 'de', 'aprendizado', 'de', 'pln', 'pode', 'ser', 'desafiador', 'porem', 'gratificante']\n",
            "['as', 'redes', 'sociais', 'sao', 'uma', 'ferramenta', 'poderosa', 'para', 'comunicacao', 'e', 'informacao']\n",
            "['eu', 'adoro', 'pizza', 'de', 'pepperoni', 'mas', 'tambem', 'amo', 'lasanha', 'a', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de sentenças\n",
        "\n",
        "textos_tokenizados_sentencas = [tokeniza_sentencas(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_sentencas:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI2bFFs3dLRJ",
        "outputId": "e3e3432f-0a38-4e6c-f248-4588f3a33012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este e um exemplo de texto com html e caracteres especiais']\n",
            "['o gato preto correu rapidamente e pulou a cerca']\n",
            "['a menina estava feliz cantando e dancando no jardim']\n",
            "['o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo']\n",
            "['a tecnologia esta avancando a cada dia e transformando nossas vidas']\n",
            "['existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades']\n",
            "['e importante estudar com dedicacao para atingir nossos objetivos']\n",
            "['o processo de aprendizado de pln pode ser desafiador porem gratificante']\n",
            "['as redes sociais sao uma ferramenta poderosa para comunicacao e informacao']\n",
            "['eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo Stop Words\n",
        "\n",
        "textos_sem_stop_words = []\n",
        "for tokens_list in textos_tokenizados_palavra: # Iterate through each list of tokens\n",
        "  tokens_sem_sw = remove_stop_words(tokens_list) # Call the function for each list\n",
        "  textos_sem_stop_words.append(tokens_sem_sw) # Append the result to the main list\n",
        "\n",
        "for lista in textos_sem_stop_words:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsssRYJVdSZ2",
        "outputId": "9bb9637d-3965-474e-ec69-2869e4968d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracteres', 'especiais']\n",
            "['gato', 'preto', 'correu', 'rapidamente', 'pulou', 'cerca']\n",
            "['menina', 'feliz', 'cantando', 'dancando', 'jardim']\n",
            "['filme', 'incrivel', 'achei', 'pouco', 'longo', 'sera', 'vale', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancando', 'cada', 'dia', 'transformando', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'mundo', 'ciencia', 'dados', 'tambem', 'muitas', 'oportunidades']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivos']\n",
            "['processo', 'aprendizado', 'pln', 'pode', 'desafiador', 'porem', 'gratificante']\n",
            "['redes', 'sociais', 'sao', 'ferramenta', 'poderosa', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanha', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando stemming aos tokens\n",
        "\n",
        "stemmed_textos = []\n",
        "for tokens_list in textos_sem_stop_words:\n",
        "    tokens_stemmed = aplica_stemming(tokens_list)\n",
        "    stemmed_textos.append(tokens_stemmed)\n",
        "\n",
        "for texto in stemmed_textos:\n",
        "  print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijGdmE3LF_bg",
        "outputId": "e7db2a7b-2c96-4b0e-ec8e-692ad0a463ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exempl', 'text', 'html', 'caract', 'espec']\n",
            "['gat', 'pret', 'corr', 'rapid', 'pul', 'cerc']\n",
            "['menin', 'feliz', 'cant', 'danc', 'jardim']\n",
            "['film', 'incri', 'ach', 'pouc', 'long', 'ser', 'val', 'pen', 'assist', 'nov']\n",
            "['tecnolog', 'avanc', 'cad', 'dia', 'transform', 'vid']\n",
            "['exist', 'muit', 'desafi', 'mund', 'cienc', 'dad', 'tamb', 'muit', 'oportun']\n",
            "['import', 'estud', 'dedicaca', 'ating', 'obje']\n",
            "['process', 'aprend', 'pln', 'pod', 'desafi', 'por', 'gratific']\n",
            "['red', 'soc', 'sao', 'ferrament', 'poder', 'comunicaca', 'informaca']\n",
            "['ador', 'pizz', 'pepperon', 'tamb', 'amo', 'lasanh', 'bolonh']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando lematização aos tokens\n",
        "\n",
        "lemm_textos = []\n",
        "for tokens_list in textos_sem_stop_words:\n",
        "    tokens_lematizados = aplica_lematizacao(tokens_list)\n",
        "    lemm_textos.append(tokens_lematizados)\n",
        "\n",
        "for texto in lemm_textos:\n",
        "  print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztq1WB8wHIEm",
        "outputId": "7058c714-5177-40e5-c836-7b2c5703c879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracter', 'especial']\n",
            "['gato', 'preto', 'correr', 'rapidamente', 'pular', 'cerca']\n",
            "['menina', 'feliz', 'cantar', 'dancar', 'Jardim']\n",
            "['filme', 'incrivel', 'achar', 'pouco', 'longo', 'sera', 'valer', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancar', 'cada', 'dia', 'transformar', 'vida']\n",
            "['existir', 'muito', 'desafio', 'mundo', 'ciencia', 'dar', 'tambem', 'muito', 'oportunidade']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivo']\n",
            "['processo', 'aprendizar', 'pln', 'poder', 'desafiador', 'por', 'gratificante']\n",
            "['rede', 'social', 'sao', 'ferramentar', 'poderoso', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanhar', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão\n",
        "\n",
        "Com este projeto, podemos:\n",
        "\n",
        "- Importar as bibliotecas necessárias para o pré-processamento de texto como `NLTK`.\n",
        "\n",
        "- Carregar um conjunto de dados de texto.\n",
        "\n",
        "- Implementar funções para remover ruídos (HTML, caracteres especiais, URLs).\n",
        "\n",
        "- Implementar funções para tokenizar o texto em palavras e frases.\n",
        "\n",
        "- Implementar uma função para converter o texto para minúsculas.\n",
        "\n",
        "- Implementar uma função para remover stop words.\n",
        "\n",
        "- Implementar funções para aplicar stemming e lematização.\n",
        "\n",
        "- Comparar os resultados de cada etapa do pré-processamento e analisar seu impacto no texto.\n",
        "\n",
        "O Pré-Processamento de texto é uma etapa crucial para qualquer projeto de PLN e com este repositório, meus futuros projetos serão impulsionados!!"
      ],
      "metadata": {
        "id": "69YyhtFdI2i6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ihqCj7RqLX7v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}