{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC5zRwJAzcBrd8ZYGxffki",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustasheep/pipeline_pre_processamento_texto/blob/main/Pr%C3%A9_processamento_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "Durante minhas aulas de processamento de linguagem natural com a IA (sim, a IA é minha professora), alcançamos a primeira atividade prática e irei construir um pipeline para meus futuros trabalhos de PLN.\n",
        "\n",
        "Eu estou ainda no Nível 1 do curso de PLN e estamos encerrando a primeira semana de estudos. Este nível tem duração de 30 dias (definido pela professora) e está sendo um aprendizado enriquecedor.\n",
        "\n",
        "Meu objetivo inicial é criar as Funções de pré-processamento:\n",
        "\n",
        "- `remover_ruido(texto)`\n",
        "- `tokenizar_palavras(texto)`\n",
        "- `tokenizar_sentencas(texto)`\n",
        "- `remover_stop_words(tokens)`\n",
        "- `aplicar_stemming(tokens)`\n",
        "- `aplicar_lematizacao(tokens)`"
      ],
      "metadata": {
        "id": "tsZqZ8xiWiav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando bibliotecas\n",
        "\n",
        "Irei carregarregar as bibliotecas `re` para fazer a remoção de ruídos, alguns pacotes do `nltk` que possui grande utilidade para pré-processamento e para o PLN em geral. E por fim, o `spacy` e o `unidecode` por terem suporte para palavras da língua portuguesa para concluir algumas etapas."
      ],
      "metadata": {
        "id": "o4sf37BWYBEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "import spacy\n",
        "\n",
        "# Baixando dependências\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvNPef37Qpvj",
        "outputId": "1e7fb18a-1f82-4e72-ec91-552b7759e79d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando função para cada etapa de pré-processamento"
      ],
      "metadata": {
        "id": "MTEiXXeMua-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remover_ruido(texto):\n",
        "  texto = re.sub(r'<.*?>', '', texto) # Remove tags HTML\n",
        "  texto = unidecode(texto)\n",
        "  texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto) # Remove caracteres especiais\n",
        "  texto = re.sub(r'http\\S+', '', texto) # Remove URLs\n",
        "  texto = re.sub(r'\\s+', ' ', texto) # Remove espaços em branco extras\n",
        "  texto = texto.strip() # Remove espaços em branco no início e no final\n",
        "  texto = texto.lower() # Converte para minúsculas\n",
        "  texto = re.sub(r'\\d+', '', texto) # Remove números\n",
        "  return texto\n",
        "\n",
        "def tokeniza_palavras(texto):\n",
        "  tokens = word_tokenize(texto)\n",
        "  return tokens\n",
        "\n",
        "def tokeniza_sentencas(texto):\n",
        "  sentencas = sent_tokenize(texto)\n",
        "  return sentencas\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "  stop_words = set(stopwords.words('portuguese'))\n",
        "  tokens_s_sw = [token for token in tokens if token not in stop_words]\n",
        "  return tokens_s_sw\n",
        "\n",
        "def aplica_stemming(tokens_s_sw):\n",
        "  stemmer = RSLPStemmer()\n",
        "  tokens_stemmed = [stemmer.stem(token) for token in tokens_s_sw]\n",
        "  return tokens_stemmed\n",
        "\n",
        "def aplica_lematizacao(tokens_s_sw):\n",
        "  nlp = spacy.load('pt_core_news_sm')\n",
        "  tokens_lematizados = [token.lemma_ for token in nlp(' '.join(tokens_s_sw))]\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "BpioeyX7RooE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto a ser trabalhado\n",
        "textos = [\n",
        "    \"<p>Este é um exemplo de texto <strong>com HTML</strong> e caracteres especiais! https://www.exemplo.com</p>\",\n",
        "    \"O GATO preto CORREU rapidamente e pulou a cerca.\",\n",
        "    \"A menina estava feliz, cantando e dançando no jardim.\",\n",
        "    \"O filme foi incrível, mas achei um pouco longo. Será que vale a pena assistir de novo?\",\n",
        "    \"A tecnologia está avançando a cada dia e transformando nossas vidas.\",\n",
        "    \"Existem muitos desafios no mundo da ciência de dados, mas também muitas oportunidades.\",\n",
        "    \"É importante estudar com dedicação para atingir nossos objetivos.\",\n",
        "    \"O processo de aprendizado de PLN pode ser desafiador, porém gratificante.\",\n",
        "    \"As redes sociais são uma ferramenta poderosa para comunicação e informação.\",\n",
        "    \"Eu adoro pizza de pepperoni, mas também amo lasanha à bolonhesa.\"\n",
        "]"
      ],
      "metadata": {
        "id": "9dqi_MFTudFi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textos_limpos = [remover_ruido(texto) for texto in textos]\n",
        "textos_limpos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I430ELKcLq4",
        "outputId": "fff0cb4b-19ed-4f66-d374-cd8a7fb76ff5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['este e um exemplo de texto com html e caracteres especiais',\n",
              " 'o gato preto correu rapidamente e pulou a cerca',\n",
              " 'a menina estava feliz cantando e dancando no jardim',\n",
              " 'o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo',\n",
              " 'a tecnologia esta avancando a cada dia e transformando nossas vidas',\n",
              " 'existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades',\n",
              " 'e importante estudar com dedicacao para atingir nossos objetivos',\n",
              " 'o processo de aprendizado de pln pode ser desafiador porem gratificante',\n",
              " 'as redes sociais sao uma ferramenta poderosa para comunicacao e informacao',\n",
              " 'eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textos_tokenizados_palavra = [tokeniza_palavras(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_palavra:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SAUWO_u6c9gH",
        "outputId": "e1371397-f1c9-4855-bf9a-db65f694bd62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este', 'e', 'um', 'exemplo', 'de', 'texto', 'com', 'html', 'e', 'caracteres', 'especiais']\n",
            "['o', 'gato', 'preto', 'correu', 'rapidamente', 'e', 'pulou', 'a', 'cerca']\n",
            "['a', 'menina', 'estava', 'feliz', 'cantando', 'e', 'dancando', 'no', 'jardim']\n",
            "['o', 'filme', 'foi', 'incrivel', 'mas', 'achei', 'um', 'pouco', 'longo', 'sera', 'que', 'vale', 'a', 'pena', 'assistir', 'de', 'novo']\n",
            "['a', 'tecnologia', 'esta', 'avancando', 'a', 'cada', 'dia', 'e', 'transformando', 'nossas', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'no', 'mundo', 'da', 'ciencia', 'de', 'dados', 'mas', 'tambem', 'muitas', 'oportunidades']\n",
            "['e', 'importante', 'estudar', 'com', 'dedicacao', 'para', 'atingir', 'nossos', 'objetivos']\n",
            "['o', 'processo', 'de', 'aprendizado', 'de', 'pln', 'pode', 'ser', 'desafiador', 'porem', 'gratificante']\n",
            "['as', 'redes', 'sociais', 'sao', 'uma', 'ferramenta', 'poderosa', 'para', 'comunicacao', 'e', 'informacao']\n",
            "['eu', 'adoro', 'pizza', 'de', 'pepperoni', 'mas', 'tambem', 'amo', 'lasanha', 'a', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textos_tokenizados_sentencas = [tokeniza_sentencas(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_sentencas:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI2bFFs3dLRJ",
        "outputId": "c4add809-e684-407f-8089-a47cb616b8bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este e um exemplo de texto com html e caracteres especiais']\n",
            "['o gato preto correu rapidamente e pulou a cerca']\n",
            "['a menina estava feliz cantando e dancando no jardim']\n",
            "['o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo']\n",
            "['a tecnologia esta avancando a cada dia e transformando nossas vidas']\n",
            "['existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades']\n",
            "['e importante estudar com dedicacao para atingir nossos objetivos']\n",
            "['o processo de aprendizado de pln pode ser desafiador porem gratificante']\n",
            "['as redes sociais sao uma ferramenta poderosa para comunicacao e informacao']\n",
            "['eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textos_sem_stop_words = []\n",
        "for tokens_list in textos_tokenizados_palavra: # Iterate through each list of tokens\n",
        "  tokens_sem_sw = remove_stop_words(tokens_list) # Call the function for each list\n",
        "  textos_sem_stop_words.append(tokens_sem_sw) # Append the result to the main list\n",
        "\n",
        "for lista in textos_sem_stop_words:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsssRYJVdSZ2",
        "outputId": "072d311a-35ed-43e7-df9a-297dcea63d6f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracteres', 'especiais']\n",
            "['gato', 'preto', 'correu', 'rapidamente', 'pulou', 'cerca']\n",
            "['menina', 'feliz', 'cantando', 'dancando', 'jardim']\n",
            "['filme', 'incrivel', 'achei', 'pouco', 'longo', 'sera', 'vale', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancando', 'cada', 'dia', 'transformando', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'mundo', 'ciencia', 'dados', 'tambem', 'muitas', 'oportunidades']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivos']\n",
            "['processo', 'aprendizado', 'pln', 'pode', 'desafiador', 'porem', 'gratificante']\n",
            "['redes', 'sociais', 'sao', 'ferramenta', 'poderosa', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanha', 'bolonhesa']\n"
          ]
        }
      ]
    }
  ]
}