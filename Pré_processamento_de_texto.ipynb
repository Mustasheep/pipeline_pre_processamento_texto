{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlyoTUB1fsfbgcoIYZpIX7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "Durante minhas aulas de processamento de linguagem natural com a IA (sim, a IA é minha professora), alcançamos a primeira atividade prática e irei construir um pipeline para meus futuros trabalhos de PLN.\n",
        "\n",
        "Eu estou ainda no Nível 1 do curso de PLN e estamos encerrando a primeira semana de estudos. Este nível tem duração de 30 dias (definido pela professora) e está sendo um aprendizado enriquecedor.\n",
        "\n",
        "Meu objetivo inicial é criar as Funções de pré-processamento:\n",
        "\n",
        "- `remover_ruido(texto)`\n",
        "- `tokenizar_palavras(texto)`\n",
        "- `tokenizar_sentencas(texto)`\n",
        "- `remover_stop_words(tokens)`\n",
        "- `aplicar_stemming(tokens)`\n",
        "- `aplicar_lematizacao(tokens)`"
      ],
      "metadata": {
        "id": "tsZqZ8xiWiav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando bibliotecas\n",
        "\n",
        "Irei carregarregar as bibliotecas `re` para fazer a remoção de ruídos, alguns pacotes do `nltk` que possui grande utilidade para pré-processamento e para o PLN em geral. E por fim, o `spacy` e o `unidecode` por terem suporte para palavras da língua portuguesa para concluir algumas etapas."
      ],
      "metadata": {
        "id": "o4sf37BWYBEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "import spacy\n",
        "\n",
        "# Baixando dependências\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvNPef37Qpvj",
        "outputId": "88fc1114-d5e8-49a1-90ea-a29ad1f27984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando função para cada etapa de pré-processamento"
      ],
      "metadata": {
        "id": "MTEiXXeMua-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remover_ruido(texto):\n",
        "  texto = re.sub(r'<.*?>', '', texto) # Remove tags HTML\n",
        "  texto = unidecode(texto)\n",
        "  texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto) # Remove caracteres especiais\n",
        "  texto = re.sub(r'http\\S+', '', texto) # Remove URLs\n",
        "  texto = re.sub(r'\\s+', ' ', texto) # Remove espaços em branco extras\n",
        "  texto = texto.strip() # Remove espaços em branco no início e no final\n",
        "  texto = texto.lower() # Converte para minúsculas\n",
        "  texto = re.sub(r'\\d+', '', texto) # Remove números\n",
        "  return texto\n",
        "\n",
        "def limpa_texto(texto):\n",
        "  texto = remover_ruido(texto)\n",
        "  return texto\n",
        "\n",
        "def tokeniza_palavras(texto):\n",
        "  tokens = word_tokenize(texto)\n",
        "  return tokens\n",
        "\n",
        "def tokeniza_sentencas(texto):\n",
        "  sentencas = sent_tokenize(texto)\n",
        "  return sentencas\n",
        "\n",
        "def remove_stop_words(tokens_s_sw,tokens):\n",
        "  stop_words = set(stopwords.words('portuguese'))\n",
        "  tokens_s_sw = [token for token in tokens if token not in stop_words]\n",
        "  return toke"
      ],
      "metadata": {
        "id": "BpioeyX7RooE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto a ser trabalhado\n",
        "textos = [\n",
        "    \"<p>Este é um exemplo de texto <strong>com HTML</strong> e caracteres especiais! https://www.exemplo.com</p>\",\n",
        "    \"O GATO preto CORREU rapidamente e pulou a cerca.\",\n",
        "    \"A menina estava feliz, cantando e dançando no jardim.\",\n",
        "    \"O filme foi incrível, mas achei um pouco longo. Será que vale a pena assistir de novo?\",\n",
        "    \"A tecnologia está avançando a cada dia e transformando nossas vidas.\",\n",
        "    \"Existem muitos desafios no mundo da ciência de dados, mas também muitas oportunidades.\",\n",
        "    \"É importante estudar com dedicação para atingir nossos objetivos.\",\n",
        "    \"O processo de aprendizado de PLN pode ser desafiador, porém gratificante.\",\n",
        "    \"As redes sociais são uma ferramenta poderosa para comunicação e informação.\",\n",
        "    \"Eu adoro pizza de pepperoni, mas também amo lasanha à bolonhesa.\"\n",
        "]"
      ],
      "metadata": {
        "id": "9dqi_MFTudFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limpando todo o texto na lista\n",
        "textos_limpos = [remover_ruido(texto) for texto in textos]\n",
        "textos_limpos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39svR3PBR921",
        "outputId": "e7555a1a-dee8-4253-d70d-9aa330ac5e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['este e um exemplo de texto com html e caracteres especiais',\n",
              " 'o gato preto correu rapidamente e pulou a cerca',\n",
              " 'a menina estava feliz cantando e dancando no jardim',\n",
              " 'o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo',\n",
              " 'a tecnologia esta avancando a cada dia e transformando nossas vidas',\n",
              " 'existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades',\n",
              " 'e importante estudar com dedicacao para atingir nossos objetivos',\n",
              " 'o processo de aprendizado de pln pode ser desafiador porem gratificante',\n",
              " 'as redes sociais sao uma ferramenta poderosa para comunicacao e informacao',\n",
              " 'eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando tokenização de palavras e senteças\n",
        "textos_tokenizados_palavra = [word_tokenize(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YUuXYSebTm4X",
        "outputId": "b17d5478-6c91-41f6-8072-0b7799a42ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este', 'e', 'um', 'exemplo', 'de', 'texto', 'com', 'html', 'e', 'caracteres', 'especiais']\n",
            "['o', 'gato', 'preto', 'correu', 'rapidamente', 'e', 'pulou', 'a', 'cerca']\n",
            "['a', 'menina', 'estava', 'feliz', 'cantando', 'e', 'dancando', 'no', 'jardim']\n",
            "['o', 'filme', 'foi', 'incrivel', 'mas', 'achei', 'um', 'pouco', 'longo', 'sera', 'que', 'vale', 'a', 'pena', 'assistir', 'de', 'novo']\n",
            "['a', 'tecnologia', 'esta', 'avancando', 'a', 'cada', 'dia', 'e', 'transformando', 'nossas', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'no', 'mundo', 'da', 'ciencia', 'de', 'dados', 'mas', 'tambem', 'muitas', 'oportunidades']\n",
            "['e', 'importante', 'estudar', 'com', 'dedicacao', 'para', 'atingir', 'nossos', 'objetivos']\n",
            "['o', 'processo', 'de', 'aprendizado', 'de', 'pln', 'pode', 'ser', 'desafiador', 'porem', 'gratificante']\n",
            "['as', 'redes', 'sociais', 'sao', 'uma', 'ferramenta', 'poderosa', 'para', 'comunicacao', 'e', 'informacao']\n",
            "['eu', 'adoro', 'pizza', 'de', 'pepperoni', 'mas', 'tambem', 'amo', 'lasanha', 'a', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textos_tokenizados_sentencas = [sent_tokenize(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_sentencas:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "envRYOTGT0nr",
        "outputId": "3323e46e-2a6a-48e1-f42b-45820b8fd7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este e um exemplo de texto com html e caracteres especiais']\n",
            "['o gato preto correu rapidamente e pulou a cerca']\n",
            "['a menina estava feliz cantando e dancando no jardim']\n",
            "['o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo']\n",
            "['a tecnologia esta avancando a cada dia e transformando nossas vidas']\n",
            "['existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades']\n",
            "['e importante estudar com dedicacao para atingir nossos objetivos']\n",
            "['o processo de aprendizado de pln pode ser desafiador porem gratificante']\n",
            "['as redes sociais sao uma ferramenta poderosa para comunicacao e informacao']\n",
            "['eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo stop words\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "textos_sem_stop_words = []\n",
        "for tokens in textos_tokenizados_palavra:\n",
        "  tokens_sem_stop_words = [token for token in tokens if token not in stop_words]\n",
        "  textos_sem_stop_words.append(tokens_sem_stop_words)"
      ],
      "metadata": {
        "id": "i20hCyjkUwwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lista in textos_sem_stop_words:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8tCmjbwVMC0",
        "outputId": "a30e4543-1938-4554-a8ea-c5aeae638a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracteres', 'especiais']\n",
            "['gato', 'preto', 'correu', 'rapidamente', 'pulou', 'cerca']\n",
            "['menina', 'feliz', 'cantando', 'dancando', 'jardim']\n",
            "['filme', 'incrivel', 'achei', 'pouco', 'longo', 'sera', 'vale', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancando', 'cada', 'dia', 'transformando', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'mundo', 'ciencia', 'dados', 'tambem', 'muitas', 'oportunidades']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivos']\n",
            "['processo', 'aprendizado', 'pln', 'pode', 'desafiador', 'porem', 'gratificante']\n",
            "['redes', 'sociais', 'sao', 'ferramenta', 'poderosa', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanha', 'bolonhesa']\n"
          ]
        }
      ]
    }
  ]
}