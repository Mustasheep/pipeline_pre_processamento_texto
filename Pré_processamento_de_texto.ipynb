{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIinGz/R/roiBe+dTLNOpN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustasheep/pre_processamento_texto/blob/main/Pr%C3%A9_processamento_de_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "Durante minhas aulas de processamento de linguagem natural com a IA (sim, a IA é minha professora), alcançamos a primeira atividade prática e irei elaborar algumas funções para meus futuros trabalhos de PLN.\n",
        "\n",
        "Atualmente estou no curso de Nível 1 de Processamento de Linguagem Natural e estamos encerrando a primeira semana de estudos. Este nível tem duração de 30 dias (definido pela professora) e está sendo um aprendizado enriquecedor.\n",
        "\n",
        "**Caso queira descobrir como essas aulas foram elaboradas, segue o artigo em meu LinkedIn:** `em breve`\n",
        "\n",
        "Meu objetivo inicial é criar as Funções de pré-processamento:\n",
        "\n",
        "- `remover_ruido(texto)`\n",
        "- `tokenizar_palavras(texto)`\n",
        "- `tokenizar_sentencas(texto)`\n",
        "- `remover_stop_words(tokens)`\n",
        "- `aplicar_stemming(tokens)`\n",
        "- `aplicar_lematizacao(tokens)`"
      ],
      "metadata": {
        "id": "tsZqZ8xiWiav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando bibliotecas\n",
        "\n",
        "Irei carregarregar as bibliotecas `re` para fazer a remoção de ruídos, alguns pacotes do `nltk` que possui grande utilidade para pré-processamento e para o PLN em geral. E por fim, o `spacy` e o `unidecode` por terem suporte para palavras da língua portuguesa para concluir algumas etapas."
      ],
      "metadata": {
        "id": "o4sf37BWYBEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode > /dev/null\n",
        "!python -m spacy download pt_core_news_sm > /dev/null"
      ],
      "metadata": {
        "id": "-rToRpK0zhwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "import spacy\n",
        "\n",
        "# Baixando dependências\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvNPef37Qpvj",
        "outputId": "e5048d1c-77ba-4cef-c14a-4e9952df5b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando função para cada etapa de pré-processamento\n",
        "As funções a seguir serão para remover o ruído (caracteres especiais, URLs, espaços em branco, números, tags HTML, acentuação e padronizar em letras minúsculas). Faremos também a tokenização das palavras e sentenças, remoção dos stop words (preposições, artigos, conjunções, etc), stemming e lematização."
      ],
      "metadata": {
        "id": "MTEiXXeMua-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remover_ruido(texto):\n",
        "  '''\n",
        "  Esta função remove Tags HTML, acentuação, caracteres\n",
        "  especiais, URLs, números, espaços em branco e\n",
        "  padroniza em letras minúsculas.\n",
        "  '''\n",
        "  texto = re.sub(r'<.*?>', '', texto) # Remove tags HTML\n",
        "  texto = unidecode(texto) # Remove acentuação preservando a letra\n",
        "  texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto) # Remove caracteres especiais\n",
        "  texto = re.sub(r'http\\S+', '', texto) # Remove URLs\n",
        "  texto = re.sub(r'\\s+', ' ', texto) # Remove espaços em branco extras\n",
        "  texto = texto.strip() # Remove espaços em branco no início e no final\n",
        "  texto = texto.lower() # Converte para minúsculas\n",
        "  texto = re.sub(r'\\d+', '', texto) # Remove números\n",
        "  return texto\n",
        "\n",
        "def tokeniza_palavras(texto):\n",
        "  '''\n",
        "  Esta função tokeniza o texto em palavras.\n",
        "  '''\n",
        "  tokens = word_tokenize(texto)\n",
        "  return tokens\n",
        "\n",
        "def tokeniza_sentencas(texto):\n",
        "  '''\n",
        "  Esta função tokeniza o texto em sentenças.\n",
        "  '''\n",
        "  sentencas = sent_tokenize(texto)\n",
        "  return sentencas\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "  '''\n",
        "  Esta função remove stop words (preposições,\n",
        "  artigos, conjunções, etc)\n",
        "  '''\n",
        "  stop_words = set(stopwords.words('portuguese'))\n",
        "  tokens_s_sw = [token for token in tokens if token not in stop_words]\n",
        "  return tokens_s_sw\n",
        "\n",
        "def aplica_stemming(tokens_s_sw):\n",
        "  '''\n",
        "  Esta função aplica stemming aos tokens.\n",
        "  '''\n",
        "  stemmer = RSLPStemmer()\n",
        "  tokens_stemmed = [stemmer.stem(token) for token in tokens_s_sw]\n",
        "  return tokens_stemmed\n",
        "\n",
        "def aplica_lematizacao(tokens_s_sw):\n",
        "  '''\n",
        "  Esta função aplica lematização aos tokens.\n",
        "  '''\n",
        "  nlp = spacy.load('pt_core_news_sm')\n",
        "  tokens_lematizados = [token.lemma_ for token in nlp(' '.join(tokens_s_sw))]\n",
        "  return tokens_lematizados"
      ],
      "metadata": {
        "id": "BpioeyX7RooE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicando as funções em uma lista de textos\n",
        "A lista a seguir foi projetada para apresentar vários textos. Os textos são manipulados para colocar em ação todas as etapas de pré-processamento para fins de testes"
      ],
      "metadata": {
        "id": "MbUlfooWDH_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Textos a serem trabalhados\n",
        "\n",
        "textos = [\n",
        "    \"<p>Este é um exemplo de texto <strong>com HTML</strong> e caracteres especiais! https://www.exemplo.com</p>\",\n",
        "    \"O GATO preto CORREU rapidamente e pulou a cerca.\",\n",
        "    \"A menina estava feliz, cantando e dançando no jardim.\",\n",
        "    \"O filme foi incrível, mas achei um pouco longo. Será que vale a pena assistir de novo?\",\n",
        "    \"A tecnologia está avançando a cada dia e transformando nossas vidas.\",\n",
        "    \"Existem muitos desafios no mundo da ciência de dados, mas também muitas oportunidades.\",\n",
        "    \"É importante estudar com dedicação para atingir nossos objetivos.\",\n",
        "    \"O processo de aprendizado de PLN pode ser desafiador, porém gratificante.\",\n",
        "    \"As redes sociais são uma ferramenta poderosa para comunicação e informação.\",\n",
        "    \"Eu adoro pizza de pepperoni, mas também amo lasanha à bolonhesa.\"\n",
        "]"
      ],
      "metadata": {
        "id": "9dqi_MFTudFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo ruídos e padronizando com letras minúsculas\n",
        "\n",
        "textos_limpos = [remover_ruido(texto) for texto in textos]\n",
        "textos_limpos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I430ELKcLq4",
        "outputId": "069df911-9fdf-44e1-bd98-c0750068b4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['este e um exemplo de texto com html e caracteres especiais',\n",
              " 'o gato preto correu rapidamente e pulou a cerca',\n",
              " 'a menina estava feliz cantando e dancando no jardim',\n",
              " 'o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo',\n",
              " 'a tecnologia esta avancando a cada dia e transformando nossas vidas',\n",
              " 'existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades',\n",
              " 'e importante estudar com dedicacao para atingir nossos objetivos',\n",
              " 'o processo de aprendizado de pln pode ser desafiador porem gratificante',\n",
              " 'as redes sociais sao uma ferramenta poderosa para comunicacao e informacao',\n",
              " 'eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de palavras\n",
        "\n",
        "textos_tokenizados_palavra = [tokeniza_palavras(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_palavra:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SAUWO_u6c9gH",
        "outputId": "ef764a73-fbbe-4896-ace0-5c7e2d2ee2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este', 'e', 'um', 'exemplo', 'de', 'texto', 'com', 'html', 'e', 'caracteres', 'especiais']\n",
            "['o', 'gato', 'preto', 'correu', 'rapidamente', 'e', 'pulou', 'a', 'cerca']\n",
            "['a', 'menina', 'estava', 'feliz', 'cantando', 'e', 'dancando', 'no', 'jardim']\n",
            "['o', 'filme', 'foi', 'incrivel', 'mas', 'achei', 'um', 'pouco', 'longo', 'sera', 'que', 'vale', 'a', 'pena', 'assistir', 'de', 'novo']\n",
            "['a', 'tecnologia', 'esta', 'avancando', 'a', 'cada', 'dia', 'e', 'transformando', 'nossas', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'no', 'mundo', 'da', 'ciencia', 'de', 'dados', 'mas', 'tambem', 'muitas', 'oportunidades']\n",
            "['e', 'importante', 'estudar', 'com', 'dedicacao', 'para', 'atingir', 'nossos', 'objetivos']\n",
            "['o', 'processo', 'de', 'aprendizado', 'de', 'pln', 'pode', 'ser', 'desafiador', 'porem', 'gratificante']\n",
            "['as', 'redes', 'sociais', 'sao', 'uma', 'ferramenta', 'poderosa', 'para', 'comunicacao', 'e', 'informacao']\n",
            "['eu', 'adoro', 'pizza', 'de', 'pepperoni', 'mas', 'tambem', 'amo', 'lasanha', 'a', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de sentenças\n",
        "\n",
        "textos_tokenizados_sentencas = [tokeniza_sentencas(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_sentencas:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI2bFFs3dLRJ",
        "outputId": "3b14fe93-578a-4b7c-e012-8fb2806ea47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este e um exemplo de texto com html e caracteres especiais']\n",
            "['o gato preto correu rapidamente e pulou a cerca']\n",
            "['a menina estava feliz cantando e dancando no jardim']\n",
            "['o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo']\n",
            "['a tecnologia esta avancando a cada dia e transformando nossas vidas']\n",
            "['existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades']\n",
            "['e importante estudar com dedicacao para atingir nossos objetivos']\n",
            "['o processo de aprendizado de pln pode ser desafiador porem gratificante']\n",
            "['as redes sociais sao uma ferramenta poderosa para comunicacao e informacao']\n",
            "['eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo Stop Words\n",
        "\n",
        "textos_sem_stop_words = []\n",
        "for tokens_list in textos_tokenizados_palavra: # Iterate through each list of tokens\n",
        "  tokens_sem_sw = remove_stop_words(tokens_list) # Call the function for each list\n",
        "  textos_sem_stop_words.append(tokens_sem_sw) # Append the result to the main list\n",
        "\n",
        "for lista in textos_sem_stop_words:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsssRYJVdSZ2",
        "outputId": "7d27940b-f5d5-4fab-fdc5-b0d9bf3c9e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracteres', 'especiais']\n",
            "['gato', 'preto', 'correu', 'rapidamente', 'pulou', 'cerca']\n",
            "['menina', 'feliz', 'cantando', 'dancando', 'jardim']\n",
            "['filme', 'incrivel', 'achei', 'pouco', 'longo', 'sera', 'vale', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancando', 'cada', 'dia', 'transformando', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'mundo', 'ciencia', 'dados', 'tambem', 'muitas', 'oportunidades']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivos']\n",
            "['processo', 'aprendizado', 'pln', 'pode', 'desafiador', 'porem', 'gratificante']\n",
            "['redes', 'sociais', 'sao', 'ferramenta', 'poderosa', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanha', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando stemming aos tokens\n",
        "\n",
        "stemmed_textos = []\n",
        "for tokens_list in textos_sem_stop_words:\n",
        "    tokens_stemmed = aplica_stemming(tokens_list)\n",
        "    stemmed_textos.append(tokens_stemmed)\n",
        "\n",
        "for texto in stemmed_textos:\n",
        "  print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijGdmE3LF_bg",
        "outputId": "8181951d-437b-48ad-e871-c7d06431579f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exempl', 'text', 'html', 'caract', 'espec']\n",
            "['gat', 'pret', 'corr', 'rapid', 'pul', 'cerc']\n",
            "['menin', 'feliz', 'cant', 'danc', 'jardim']\n",
            "['film', 'incri', 'ach', 'pouc', 'long', 'ser', 'val', 'pen', 'assist', 'nov']\n",
            "['tecnolog', 'avanc', 'cad', 'dia', 'transform', 'vid']\n",
            "['exist', 'muit', 'desafi', 'mund', 'cienc', 'dad', 'tamb', 'muit', 'oportun']\n",
            "['import', 'estud', 'dedicaca', 'ating', 'obje']\n",
            "['process', 'aprend', 'pln', 'pod', 'desafi', 'por', 'gratific']\n",
            "['red', 'soc', 'sao', 'ferrament', 'poder', 'comunicaca', 'informaca']\n",
            "['ador', 'pizz', 'pepperon', 'tamb', 'amo', 'lasanh', 'bolonh']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando lematização aos tokens\n",
        "\n",
        "lemm_textos = []\n",
        "for tokens_list in textos_sem_stop_words:\n",
        "    tokens_lematizados = aplica_lematizacao(tokens_list)\n",
        "    lemm_textos.append(tokens_lematizados)\n",
        "\n",
        "for texto in lemm_textos:\n",
        "  print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztq1WB8wHIEm",
        "outputId": "f8b2adbb-a4ef-4b55-9b0c-8a334c4affad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracter', 'especial']\n",
            "['gato', 'preto', 'correr', 'rapidamente', 'pular', 'cerca']\n",
            "['menina', 'feliz', 'cantar', 'dancar', 'Jardim']\n",
            "['filme', 'incrivel', 'achar', 'pouco', 'longo', 'sera', 'valer', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancar', 'cada', 'dia', 'transformar', 'vida']\n",
            "['existir', 'muito', 'desafio', 'mundo', 'ciencia', 'dar', 'tambem', 'muito', 'oportunidade']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivo']\n",
            "['processo', 'aprendizar', 'pln', 'poder', 'desafiador', 'por', 'gratificante']\n",
            "['rede', 'social', 'sao', 'ferramentar', 'poderoso', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanhar', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisando o impacto de cada etapa do Pré-Processamento\n",
        "Para concluir o projeto, vamos rever cada etapa de cada frase separadamente, para podermos analisar o impacto de cada ação durante o pré-processamento de textos, o que irá abrir uma gama de possibilidades para projetos futuros."
      ],
      "metadata": {
        "id": "YZtDHQtAzpeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for texto in range(len(textos)):\n",
        "  print(f\"Texto Original: {textos[texto]}\")\n",
        "  print(f\"Texto Limpo: {textos_limpos[texto]}\")\n",
        "  print(f\"Tokens de Palavras: {textos_tokenizados_palavra[texto]}\")\n",
        "  print(f\"Tokens de Sentenças: {textos_tokenizados_sentencas[texto]}\")\n",
        "  print(f\"Tokens sem Stop Words: {textos_sem_stop_words[texto]}\")\n",
        "  print(f\"Tokens Stemmed: {stemmed_textos[texto]}\")\n",
        "  print(f\"Tokens Lematizados: {lemm_textos[texto]}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkoOYNwVzve9",
        "outputId": "2dc23dd5-774d-429d-b85d-8871504893c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto Original: <p>Este é um exemplo de texto <strong>com HTML</strong> e caracteres especiais! https://www.exemplo.com</p>\n",
            "Texto Limpo: este e um exemplo de texto com html e caracteres especiais\n",
            "Tokens de Palavras: ['este', 'e', 'um', 'exemplo', 'de', 'texto', 'com', 'html', 'e', 'caracteres', 'especiais']\n",
            "Tokens de Sentenças: ['este e um exemplo de texto com html e caracteres especiais']\n",
            "Tokens sem Stop Words: ['exemplo', 'texto', 'html', 'caracteres', 'especiais']\n",
            "Tokens Stemmed: ['exempl', 'text', 'html', 'caract', 'espec']\n",
            "Tokens Lematizados: ['exemplo', 'texto', 'html', 'caracter', 'especial']\n",
            "\n",
            "\n",
            "Texto Original: O GATO preto CORREU rapidamente e pulou a cerca.\n",
            "Texto Limpo: o gato preto correu rapidamente e pulou a cerca\n",
            "Tokens de Palavras: ['o', 'gato', 'preto', 'correu', 'rapidamente', 'e', 'pulou', 'a', 'cerca']\n",
            "Tokens de Sentenças: ['o gato preto correu rapidamente e pulou a cerca']\n",
            "Tokens sem Stop Words: ['gato', 'preto', 'correu', 'rapidamente', 'pulou', 'cerca']\n",
            "Tokens Stemmed: ['gat', 'pret', 'corr', 'rapid', 'pul', 'cerc']\n",
            "Tokens Lematizados: ['gato', 'preto', 'correr', 'rapidamente', 'pular', 'cerca']\n",
            "\n",
            "\n",
            "Texto Original: A menina estava feliz, cantando e dançando no jardim.\n",
            "Texto Limpo: a menina estava feliz cantando e dancando no jardim\n",
            "Tokens de Palavras: ['a', 'menina', 'estava', 'feliz', 'cantando', 'e', 'dancando', 'no', 'jardim']\n",
            "Tokens de Sentenças: ['a menina estava feliz cantando e dancando no jardim']\n",
            "Tokens sem Stop Words: ['menina', 'feliz', 'cantando', 'dancando', 'jardim']\n",
            "Tokens Stemmed: ['menin', 'feliz', 'cant', 'danc', 'jardim']\n",
            "Tokens Lematizados: ['menina', 'feliz', 'cantar', 'dancar', 'Jardim']\n",
            "\n",
            "\n",
            "Texto Original: O filme foi incrível, mas achei um pouco longo. Será que vale a pena assistir de novo?\n",
            "Texto Limpo: o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo\n",
            "Tokens de Palavras: ['o', 'filme', 'foi', 'incrivel', 'mas', 'achei', 'um', 'pouco', 'longo', 'sera', 'que', 'vale', 'a', 'pena', 'assistir', 'de', 'novo']\n",
            "Tokens de Sentenças: ['o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo']\n",
            "Tokens sem Stop Words: ['filme', 'incrivel', 'achei', 'pouco', 'longo', 'sera', 'vale', 'pena', 'assistir', 'novo']\n",
            "Tokens Stemmed: ['film', 'incri', 'ach', 'pouc', 'long', 'ser', 'val', 'pen', 'assist', 'nov']\n",
            "Tokens Lematizados: ['filme', 'incrivel', 'achar', 'pouco', 'longo', 'sera', 'valer', 'pena', 'assistir', 'novo']\n",
            "\n",
            "\n",
            "Texto Original: A tecnologia está avançando a cada dia e transformando nossas vidas.\n",
            "Texto Limpo: a tecnologia esta avancando a cada dia e transformando nossas vidas\n",
            "Tokens de Palavras: ['a', 'tecnologia', 'esta', 'avancando', 'a', 'cada', 'dia', 'e', 'transformando', 'nossas', 'vidas']\n",
            "Tokens de Sentenças: ['a tecnologia esta avancando a cada dia e transformando nossas vidas']\n",
            "Tokens sem Stop Words: ['tecnologia', 'avancando', 'cada', 'dia', 'transformando', 'vidas']\n",
            "Tokens Stemmed: ['tecnolog', 'avanc', 'cad', 'dia', 'transform', 'vid']\n",
            "Tokens Lematizados: ['tecnologia', 'avancar', 'cada', 'dia', 'transformar', 'vida']\n",
            "\n",
            "\n",
            "Texto Original: Existem muitos desafios no mundo da ciência de dados, mas também muitas oportunidades.\n",
            "Texto Limpo: existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades\n",
            "Tokens de Palavras: ['existem', 'muitos', 'desafios', 'no', 'mundo', 'da', 'ciencia', 'de', 'dados', 'mas', 'tambem', 'muitas', 'oportunidades']\n",
            "Tokens de Sentenças: ['existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades']\n",
            "Tokens sem Stop Words: ['existem', 'muitos', 'desafios', 'mundo', 'ciencia', 'dados', 'tambem', 'muitas', 'oportunidades']\n",
            "Tokens Stemmed: ['exist', 'muit', 'desafi', 'mund', 'cienc', 'dad', 'tamb', 'muit', 'oportun']\n",
            "Tokens Lematizados: ['existir', 'muito', 'desafio', 'mundo', 'ciencia', 'dar', 'tambem', 'muito', 'oportunidade']\n",
            "\n",
            "\n",
            "Texto Original: É importante estudar com dedicação para atingir nossos objetivos.\n",
            "Texto Limpo: e importante estudar com dedicacao para atingir nossos objetivos\n",
            "Tokens de Palavras: ['e', 'importante', 'estudar', 'com', 'dedicacao', 'para', 'atingir', 'nossos', 'objetivos']\n",
            "Tokens de Sentenças: ['e importante estudar com dedicacao para atingir nossos objetivos']\n",
            "Tokens sem Stop Words: ['importante', 'estudar', 'dedicacao', 'atingir', 'objetivos']\n",
            "Tokens Stemmed: ['import', 'estud', 'dedicaca', 'ating', 'obje']\n",
            "Tokens Lematizados: ['importante', 'estudar', 'dedicacao', 'atingir', 'objetivo']\n",
            "\n",
            "\n",
            "Texto Original: O processo de aprendizado de PLN pode ser desafiador, porém gratificante.\n",
            "Texto Limpo: o processo de aprendizado de pln pode ser desafiador porem gratificante\n",
            "Tokens de Palavras: ['o', 'processo', 'de', 'aprendizado', 'de', 'pln', 'pode', 'ser', 'desafiador', 'porem', 'gratificante']\n",
            "Tokens de Sentenças: ['o processo de aprendizado de pln pode ser desafiador porem gratificante']\n",
            "Tokens sem Stop Words: ['processo', 'aprendizado', 'pln', 'pode', 'desafiador', 'porem', 'gratificante']\n",
            "Tokens Stemmed: ['process', 'aprend', 'pln', 'pod', 'desafi', 'por', 'gratific']\n",
            "Tokens Lematizados: ['processo', 'aprendizar', 'pln', 'poder', 'desafiador', 'por', 'gratificante']\n",
            "\n",
            "\n",
            "Texto Original: As redes sociais são uma ferramenta poderosa para comunicação e informação.\n",
            "Texto Limpo: as redes sociais sao uma ferramenta poderosa para comunicacao e informacao\n",
            "Tokens de Palavras: ['as', 'redes', 'sociais', 'sao', 'uma', 'ferramenta', 'poderosa', 'para', 'comunicacao', 'e', 'informacao']\n",
            "Tokens de Sentenças: ['as redes sociais sao uma ferramenta poderosa para comunicacao e informacao']\n",
            "Tokens sem Stop Words: ['redes', 'sociais', 'sao', 'ferramenta', 'poderosa', 'comunicacao', 'informacao']\n",
            "Tokens Stemmed: ['red', 'soc', 'sao', 'ferrament', 'poder', 'comunicaca', 'informaca']\n",
            "Tokens Lematizados: ['rede', 'social', 'sao', 'ferramentar', 'poderoso', 'comunicacao', 'informacao']\n",
            "\n",
            "\n",
            "Texto Original: Eu adoro pizza de pepperoni, mas também amo lasanha à bolonhesa.\n",
            "Texto Limpo: eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa\n",
            "Tokens de Palavras: ['eu', 'adoro', 'pizza', 'de', 'pepperoni', 'mas', 'tambem', 'amo', 'lasanha', 'a', 'bolonhesa']\n",
            "Tokens de Sentenças: ['eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']\n",
            "Tokens sem Stop Words: ['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanha', 'bolonhesa']\n",
            "Tokens Stemmed: ['ador', 'pizz', 'pepperon', 'tamb', 'amo', 'lasanh', 'bolonh']\n",
            "Tokens Lematizados: ['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanhar', 'bolonhesa']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão\n",
        "\n",
        "Com este projeto, podemos:\n",
        "\n",
        "- Importar as bibliotecas necessárias para o pré-processamento de texto como `NLTK`.\n",
        "\n",
        "- Carregar um conjunto de dados de texto.\n",
        "\n",
        "- Implementar funções para remover ruídos (HTML, caracteres especiais, URLs).\n",
        "\n",
        "- Implementar funções para tokenizar o texto em palavras e frases.\n",
        "\n",
        "- Implementar uma função para converter o texto para minúsculas.\n",
        "\n",
        "- Implementar uma função para remover stop words.\n",
        "\n",
        "- Implementar funções para aplicar stemming e lematização.\n",
        "\n",
        "- Comparar os resultados de cada etapa do pré-processamento e analisar seu impacto no texto."
      ],
      "metadata": {
        "id": "69YyhtFdI2i6"
      }
    }
  ]
}