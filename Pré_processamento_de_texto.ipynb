{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJ8qJT0q9J69wbKgoYHpp0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "Durante minhas aulas de processamento de linguagem natural com a IA (sim, a IA é minha professora), alcançamos a primeira atividade prática e irei construir um pipeline para meus futuros trabalhos de PLN.\n",
        "\n",
        "Eu estou ainda no Nível 1 do curso de PLN e estamos encerrando a primeira semana de estudos. Este nível tem duração de 30 dias (definido pela professora) e está sendo um aprendizado enriquecedor.\n",
        "\n",
        "Meu objetivo inicial é criar as Funções de pré-processamento:\n",
        "\n",
        "- `remover_ruido(texto)`\n",
        "- `tokenizar_palavras(texto)`\n",
        "- `tokenizar_sentencas(texto)`\n",
        "- `remover_stop_words(tokens)`\n",
        "- `aplicar_stemming(tokens)`\n",
        "- `aplicar_lematizacao(tokens)`"
      ],
      "metadata": {
        "id": "tsZqZ8xiWiav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando bibliotecas\n",
        "\n",
        "Irei carregarregar as bibliotecas `re` para fazer a remoção de ruídos, alguns pacotes do `nltk` que possui grande utilidade para pré-processamento e para o PLN em geral. E por fim, o `spacy` e o `unidecode` por terem suporte para palavras da língua portuguesa para concluir algumas etapas."
      ],
      "metadata": {
        "id": "o4sf37BWYBEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8TyyQeBHYkW",
        "outputId": "e62a6a60-6fac-43dc-eb80-dbf9818b1d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "import spacy\n",
        "\n",
        "# Baixando dependências\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvNPef37Qpvj",
        "outputId": "1203be1c-128d-4d8e-e61e-09059c274da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando função para cada etapa de pré-processamento\n",
        "As funções a seguir serão para remover o ruído (caracteres especiais, URLs, espaços em branco, números, tags HTML, acentuação e padronizar em letras minúsculas). Faremos também a tokenização das palavras e sentenças, remoção dos stop words (preposições, artigos, conjunções, etc), stemming e lematização."
      ],
      "metadata": {
        "id": "MTEiXXeMua-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remover_ruido(texto):\n",
        "  texto = re.sub(r'<.*?>', '', texto) # Remove tags HTML\n",
        "  texto = unidecode(texto) # Remove acentuação preservando a letra\n",
        "  texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto) # Remove caracteres especiais\n",
        "  texto = re.sub(r'http\\S+', '', texto) # Remove URLs\n",
        "  texto = re.sub(r'\\s+', ' ', texto) # Remove espaços em branco extras\n",
        "  texto = texto.strip() # Remove espaços em branco no início e no final\n",
        "  texto = texto.lower() # Converte para minúsculas\n",
        "  texto = re.sub(r'\\d+', '', texto) # Remove números\n",
        "  return texto\n",
        "\n",
        "def tokeniza_palavras(texto):\n",
        "  tokens = word_tokenize(texto)\n",
        "  return tokens\n",
        "\n",
        "def tokeniza_sentencas(texto):\n",
        "  sentencas = sent_tokenize(texto)\n",
        "  return sentencas\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "  stop_words = set(stopwords.words('portuguese'))\n",
        "  tokens_s_sw = [token for token in tokens if token not in stop_words]\n",
        "  return tokens_s_sw\n",
        "\n",
        "def aplica_stemming(tokens_s_sw):\n",
        "  stemmer = RSLPStemmer()\n",
        "  tokens_stemmed = [stemmer.stem(token) for token in tokens_s_sw]\n",
        "  return tokens_stemmed\n",
        "\n",
        "def aplica_lematizacao(tokens_s_sw):\n",
        "  nlp = spacy.load('pt_core_news_sm')\n",
        "  tokens_lematizados = [token.lemma_ for token in nlp(' '.join(tokens_s_sw))]\n",
        "  return tokens_lematizados"
      ],
      "metadata": {
        "id": "BpioeyX7RooE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicando as funções em uma lista de textos"
      ],
      "metadata": {
        "id": "MbUlfooWDH_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Textos a serem trabalhados\n",
        "\n",
        "textos = [\n",
        "    \"<p>Este é um exemplo de texto <strong>com HTML</strong> e caracteres especiais! https://www.exemplo.com</p>\",\n",
        "    \"O GATO preto CORREU rapidamente e pulou a cerca.\",\n",
        "    \"A menina estava feliz, cantando e dançando no jardim.\",\n",
        "    \"O filme foi incrível, mas achei um pouco longo. Será que vale a pena assistir de novo?\",\n",
        "    \"A tecnologia está avançando a cada dia e transformando nossas vidas.\",\n",
        "    \"Existem muitos desafios no mundo da ciência de dados, mas também muitas oportunidades.\",\n",
        "    \"É importante estudar com dedicação para atingir nossos objetivos.\",\n",
        "    \"O processo de aprendizado de PLN pode ser desafiador, porém gratificante.\",\n",
        "    \"As redes sociais são uma ferramenta poderosa para comunicação e informação.\",\n",
        "    \"Eu adoro pizza de pepperoni, mas também amo lasanha à bolonhesa.\"\n",
        "]"
      ],
      "metadata": {
        "id": "9dqi_MFTudFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo ruídos e padronizando com letras minúsculas\n",
        "\n",
        "textos_limpos = [remover_ruido(texto) for texto in textos]\n",
        "textos_limpos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I430ELKcLq4",
        "outputId": "889dc1a7-f3bf-4e19-bc6d-c5f7f2bc30be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['este e um exemplo de texto com html e caracteres especiais',\n",
              " 'o gato preto correu rapidamente e pulou a cerca',\n",
              " 'a menina estava feliz cantando e dancando no jardim',\n",
              " 'o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo',\n",
              " 'a tecnologia esta avancando a cada dia e transformando nossas vidas',\n",
              " 'existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades',\n",
              " 'e importante estudar com dedicacao para atingir nossos objetivos',\n",
              " 'o processo de aprendizado de pln pode ser desafiador porem gratificante',\n",
              " 'as redes sociais sao uma ferramenta poderosa para comunicacao e informacao',\n",
              " 'eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de palavras\n",
        "\n",
        "textos_tokenizados_palavra = [tokeniza_palavras(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_palavra:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SAUWO_u6c9gH",
        "outputId": "aed8bf96-0d3f-4b96-dbdc-48d03fe55459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este', 'e', 'um', 'exemplo', 'de', 'texto', 'com', 'html', 'e', 'caracteres', 'especiais']\n",
            "['o', 'gato', 'preto', 'correu', 'rapidamente', 'e', 'pulou', 'a', 'cerca']\n",
            "['a', 'menina', 'estava', 'feliz', 'cantando', 'e', 'dancando', 'no', 'jardim']\n",
            "['o', 'filme', 'foi', 'incrivel', 'mas', 'achei', 'um', 'pouco', 'longo', 'sera', 'que', 'vale', 'a', 'pena', 'assistir', 'de', 'novo']\n",
            "['a', 'tecnologia', 'esta', 'avancando', 'a', 'cada', 'dia', 'e', 'transformando', 'nossas', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'no', 'mundo', 'da', 'ciencia', 'de', 'dados', 'mas', 'tambem', 'muitas', 'oportunidades']\n",
            "['e', 'importante', 'estudar', 'com', 'dedicacao', 'para', 'atingir', 'nossos', 'objetivos']\n",
            "['o', 'processo', 'de', 'aprendizado', 'de', 'pln', 'pode', 'ser', 'desafiador', 'porem', 'gratificante']\n",
            "['as', 'redes', 'sociais', 'sao', 'uma', 'ferramenta', 'poderosa', 'para', 'comunicacao', 'e', 'informacao']\n",
            "['eu', 'adoro', 'pizza', 'de', 'pepperoni', 'mas', 'tambem', 'amo', 'lasanha', 'a', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de sentenças\n",
        "\n",
        "textos_tokenizados_sentencas = [tokeniza_sentencas(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_sentencas:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI2bFFs3dLRJ",
        "outputId": "e3e3432f-0a38-4e6c-f248-4588f3a33012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este e um exemplo de texto com html e caracteres especiais']\n",
            "['o gato preto correu rapidamente e pulou a cerca']\n",
            "['a menina estava feliz cantando e dancando no jardim']\n",
            "['o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo']\n",
            "['a tecnologia esta avancando a cada dia e transformando nossas vidas']\n",
            "['existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades']\n",
            "['e importante estudar com dedicacao para atingir nossos objetivos']\n",
            "['o processo de aprendizado de pln pode ser desafiador porem gratificante']\n",
            "['as redes sociais sao uma ferramenta poderosa para comunicacao e informacao']\n",
            "['eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo Stop Words\n",
        "\n",
        "textos_sem_stop_words = []\n",
        "for tokens_list in textos_tokenizados_palavra: # Iterate through each list of tokens\n",
        "  tokens_sem_sw = remove_stop_words(tokens_list) # Call the function for each list\n",
        "  textos_sem_stop_words.append(tokens_sem_sw) # Append the result to the main list\n",
        "\n",
        "for lista in textos_sem_stop_words:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsssRYJVdSZ2",
        "outputId": "9bb9637d-3965-474e-ec69-2869e4968d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracteres', 'especiais']\n",
            "['gato', 'preto', 'correu', 'rapidamente', 'pulou', 'cerca']\n",
            "['menina', 'feliz', 'cantando', 'dancando', 'jardim']\n",
            "['filme', 'incrivel', 'achei', 'pouco', 'longo', 'sera', 'vale', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancando', 'cada', 'dia', 'transformando', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'mundo', 'ciencia', 'dados', 'tambem', 'muitas', 'oportunidades']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivos']\n",
            "['processo', 'aprendizado', 'pln', 'pode', 'desafiador', 'porem', 'gratificante']\n",
            "['redes', 'sociais', 'sao', 'ferramenta', 'poderosa', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanha', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando derivação aos tokens\n",
        "\n",
        "stemmed_textos = []\n",
        "for tokens_list in textos_sem_stop_words:\n",
        "    tokens_stemmed = aplica_stemming(tokens_list)\n",
        "    stemmed_textos.append(tokens_stemmed)\n",
        "\n",
        "for texto in stemmed_textos:\n",
        "  print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijGdmE3LF_bg",
        "outputId": "e7db2a7b-2c96-4b0e-ec8e-692ad0a463ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exempl', 'text', 'html', 'caract', 'espec']\n",
            "['gat', 'pret', 'corr', 'rapid', 'pul', 'cerc']\n",
            "['menin', 'feliz', 'cant', 'danc', 'jardim']\n",
            "['film', 'incri', 'ach', 'pouc', 'long', 'ser', 'val', 'pen', 'assist', 'nov']\n",
            "['tecnolog', 'avanc', 'cad', 'dia', 'transform', 'vid']\n",
            "['exist', 'muit', 'desafi', 'mund', 'cienc', 'dad', 'tamb', 'muit', 'oportun']\n",
            "['import', 'estud', 'dedicaca', 'ating', 'obje']\n",
            "['process', 'aprend', 'pln', 'pod', 'desafi', 'por', 'gratific']\n",
            "['red', 'soc', 'sao', 'ferrament', 'poder', 'comunicaca', 'informaca']\n",
            "['ador', 'pizz', 'pepperon', 'tamb', 'amo', 'lasanh', 'bolonh']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando lematização aos tokens\n",
        "\n",
        "lemm_textos = []\n",
        "for tokens_list in textos_sem_stop_words:\n",
        "    tokens_lematizados = aplica_lematizacao(tokens_list)\n",
        "    lemm_textos.append(tokens_lematizados)\n",
        "\n",
        "for texto in lemm_textos:\n",
        "  print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztq1WB8wHIEm",
        "outputId": "7058c714-5177-40e5-c836-7b2c5703c879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracter', 'especial']\n",
            "['gato', 'preto', 'correr', 'rapidamente', 'pular', 'cerca']\n",
            "['menina', 'feliz', 'cantar', 'dancar', 'Jardim']\n",
            "['filme', 'incrivel', 'achar', 'pouco', 'longo', 'sera', 'valer', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancar', 'cada', 'dia', 'transformar', 'vida']\n",
            "['existir', 'muito', 'desafio', 'mundo', 'ciencia', 'dar', 'tambem', 'muito', 'oportunidade']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivo']\n",
            "['processo', 'aprendizar', 'pln', 'poder', 'desafiador', 'por', 'gratificante']\n",
            "['rede', 'social', 'sao', 'ferramentar', 'poderoso', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanhar', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusão\n",
        "\n",
        "Com este projeto, podemos fazer o pré-processamento de textos e esse é um passo crucial para meus futuros projetos com PLN"
      ],
      "metadata": {
        "id": "69YyhtFdI2i6"
      }
    }
  ]
}