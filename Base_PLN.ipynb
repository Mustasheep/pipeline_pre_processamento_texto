{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "s_1BCD9F1jM5",
        "sUd9-HXs1bOJ",
        "D_SdEjCI12eY",
        "4qc-A1tW3sGb"
      ],
      "authorship_tag": "ABX9TyMCSEMX6fqTg1QlF02NWFVM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mustasheep/PLN-Structure/blob/main/Base_PLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "Durante minhas aulas de processamento de linguagem natural com a IA (sim, a IA é minha professora), alcançamos a primeira atividade prática e irei elaborar algumas funções para meus futuros trabalhos de PLN.\n",
        "\n",
        "Atualmente estou no curso de Nível 1 de Processamento de Linguagem Natural e estamos encerrando a primeira semana de estudos. Este nível tem duração de 30 dias (definido pela professora), com exercícios, provas e projetos práticos.\n",
        "\n",
        "**Caso queira descobrir como essas aulas foram elaboradas, segue o artigo em meu LinkedIn  ⬇**\n",
        "\n",
        "https://www.linkedin.com/pulse/aula-particular-com-ia-um-guia-pr%C3%A1tico-thiago-de-assis-imwgf/\n",
        "\n"
      ],
      "metadata": {
        "id": "tsZqZ8xiWiav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivos\n",
        "\n",
        "1. Criar as Funções de pré-processamento:\n",
        "\n",
        "  - `remover_ruido(texto)`\n",
        "  - `tokenizar_palavras(texto)`\n",
        "  - `tokenizar_sentencas(texto)`\n",
        "  - `remover_stop_words(tokens)`\n",
        "  - `aplicar_stemming(tokens)`\n",
        "  - `aplicar_lematizacao(tokens)`\n",
        "\n",
        "2. Documentar sobre como transformar textos em números:\n",
        "  - `BoW`\n",
        "  - `TF-IDF`\n",
        "\n",
        "3. Apresentar modelos de linguagem N-gramas\n",
        "  - N-Gramas\n",
        "  - Probabilidade\n",
        "  - Suavização"
      ],
      "metadata": {
        "id": "QiPBrDrwzfgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-Processamento de texto"
      ],
      "metadata": {
        "id": "s_1BCD9F1jM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carregando bibliotecas\n",
        "\n",
        "Irei carregarregar as bibliotecas `re` para fazer a remoção de ruídos, alguns pacotes do `nltk` que possui grande utilidade para pré-processamento e para o PLN em geral. E por fim, o `spacy` e o `unidecode` por terem suporte para palavras da língua portuguesa para concluir algumas etapas."
      ],
      "metadata": {
        "id": "o4sf37BWYBEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode > /dev/null\n",
        "!python -m spacy download pt_core_news_sm > /dev/null"
      ],
      "metadata": {
        "id": "-rToRpK0zhwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import RSLPStemmer\n",
        "import spacy\n",
        "\n",
        "# Baixando dependências\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvNPef37Qpvj",
        "outputId": "e5048d1c-77ba-4cef-c14a-4e9952df5b74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando função para cada etapa de pré-processamento\n",
        "As funções a seguir serão para remover o ruído (caracteres especiais, URLs, espaços em branco, números, tags HTML, acentuação e padronizar em letras minúsculas). Faremos também a tokenização das palavras e sentenças, remoção dos stop words (preposições, artigos, conjunções, etc), stemming e lematização."
      ],
      "metadata": {
        "id": "MTEiXXeMua-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remover_ruido(texto):\n",
        "  '''\n",
        "  Esta função remove Tags HTML, acentuação, caracteres\n",
        "  especiais, URLs, números, espaços em branco e\n",
        "  padroniza em letras minúsculas.\n",
        "  '''\n",
        "  texto = re.sub(r'<.*?>', '', texto) # Remove tags HTML\n",
        "  texto = unidecode(texto) # Remove acentuação preservando a letra\n",
        "  texto = re.sub(r'[^a-zA-Z0-9\\s]', '', texto) # Remove caracteres especiais\n",
        "  texto = re.sub(r'http\\S+', '', texto) # Remove URLs\n",
        "  texto = re.sub(r'\\s+', ' ', texto) # Remove espaços em branco extras\n",
        "  texto = texto.strip() # Remove espaços em branco no início e no final\n",
        "  texto = texto.lower() # Converte para minúsculas\n",
        "  texto = re.sub(r'\\d+', '', texto) # Remove números\n",
        "  return texto\n",
        "\n",
        "def tokeniza_palavras(texto):\n",
        "  '''\n",
        "  Esta função tokeniza o texto em palavras.\n",
        "  '''\n",
        "  tokens = word_tokenize(texto)\n",
        "  return tokens\n",
        "\n",
        "def tokeniza_sentencas(texto):\n",
        "  '''\n",
        "  Esta função tokeniza o texto em sentenças.\n",
        "  '''\n",
        "  sentencas = sent_tokenize(texto)\n",
        "  return sentencas\n",
        "\n",
        "def remove_stop_words(tokens):\n",
        "  '''\n",
        "  Esta função remove stop words (preposições,\n",
        "  artigos, conjunções, etc)\n",
        "  '''\n",
        "  stop_words = set(stopwords.words('portuguese'))\n",
        "  tokens_s_sw = [token for token in tokens if token not in stop_words]\n",
        "  return tokens_s_sw\n",
        "\n",
        "def aplica_stemming(tokens_s_sw):\n",
        "  '''\n",
        "  Esta função aplica stemming aos tokens.\n",
        "  '''\n",
        "  stemmer = RSLPStemmer()\n",
        "  tokens_stemmed = [stemmer.stem(token) for token in tokens_s_sw]\n",
        "  return tokens_stemmed\n",
        "\n",
        "def aplica_lematizacao(tokens_s_sw):\n",
        "  '''\n",
        "  Esta função aplica lematização aos tokens.\n",
        "  '''\n",
        "  nlp = spacy.load('pt_core_news_sm')\n",
        "  tokens_lematizados = [token.lemma_ for token in nlp(' '.join(tokens_s_sw))]\n",
        "  return tokens_lematizados"
      ],
      "metadata": {
        "id": "BpioeyX7RooE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicando as funções em uma lista de textos\n",
        "A lista a seguir foi projetada para apresentar vários textos. Os textos são manipulados para colocar em ação todas as etapas de pré-processamento para fins de testes"
      ],
      "metadata": {
        "id": "MbUlfooWDH_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Textos a serem trabalhados\n",
        "\n",
        "textos = [\n",
        "    \"<p>Este é um exemplo de texto <strong>com HTML</strong> e caracteres especiais! https://www.exemplo.com</p>\",\n",
        "    \"O GATO preto CORREU rapidamente e pulou a cerca.\",\n",
        "    \"A menina estava feliz, cantando e dançando no jardim.\",\n",
        "    \"O filme foi incrível, mas achei um pouco longo. Será que vale a pena assistir de novo?\",\n",
        "    \"A tecnologia está avançando a cada dia e transformando nossas vidas.\",\n",
        "    \"Existem muitos desafios no mundo da ciência de dados, mas também muitas oportunidades.\",\n",
        "    \"É importante estudar com dedicação para atingir nossos objetivos.\",\n",
        "    \"O processo de aprendizado de PLN pode ser desafiador, porém gratificante.\",\n",
        "    \"As redes sociais são uma ferramenta poderosa para comunicação e informação.\",\n",
        "    \"Eu adoro pizza de pepperoni, mas também amo lasanha à bolonhesa.\"\n",
        "]"
      ],
      "metadata": {
        "id": "9dqi_MFTudFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo ruídos e padronizando com letras minúsculas\n",
        "\n",
        "textos_limpos = [remover_ruido(texto) for texto in textos]\n",
        "textos_limpos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I430ELKcLq4",
        "outputId": "069df911-9fdf-44e1-bd98-c0750068b4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['este e um exemplo de texto com html e caracteres especiais',\n",
              " 'o gato preto correu rapidamente e pulou a cerca',\n",
              " 'a menina estava feliz cantando e dancando no jardim',\n",
              " 'o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo',\n",
              " 'a tecnologia esta avancando a cada dia e transformando nossas vidas',\n",
              " 'existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades',\n",
              " 'e importante estudar com dedicacao para atingir nossos objetivos',\n",
              " 'o processo de aprendizado de pln pode ser desafiador porem gratificante',\n",
              " 'as redes sociais sao uma ferramenta poderosa para comunicacao e informacao',\n",
              " 'eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de palavras\n",
        "\n",
        "textos_tokenizados_palavra = [tokeniza_palavras(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_palavra:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SAUWO_u6c9gH",
        "outputId": "ef764a73-fbbe-4896-ace0-5c7e2d2ee2fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este', 'e', 'um', 'exemplo', 'de', 'texto', 'com', 'html', 'e', 'caracteres', 'especiais']\n",
            "['o', 'gato', 'preto', 'correu', 'rapidamente', 'e', 'pulou', 'a', 'cerca']\n",
            "['a', 'menina', 'estava', 'feliz', 'cantando', 'e', 'dancando', 'no', 'jardim']\n",
            "['o', 'filme', 'foi', 'incrivel', 'mas', 'achei', 'um', 'pouco', 'longo', 'sera', 'que', 'vale', 'a', 'pena', 'assistir', 'de', 'novo']\n",
            "['a', 'tecnologia', 'esta', 'avancando', 'a', 'cada', 'dia', 'e', 'transformando', 'nossas', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'no', 'mundo', 'da', 'ciencia', 'de', 'dados', 'mas', 'tambem', 'muitas', 'oportunidades']\n",
            "['e', 'importante', 'estudar', 'com', 'dedicacao', 'para', 'atingir', 'nossos', 'objetivos']\n",
            "['o', 'processo', 'de', 'aprendizado', 'de', 'pln', 'pode', 'ser', 'desafiador', 'porem', 'gratificante']\n",
            "['as', 'redes', 'sociais', 'sao', 'uma', 'ferramenta', 'poderosa', 'para', 'comunicacao', 'e', 'informacao']\n",
            "['eu', 'adoro', 'pizza', 'de', 'pepperoni', 'mas', 'tambem', 'amo', 'lasanha', 'a', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokens de sentenças\n",
        "\n",
        "textos_tokenizados_sentencas = [tokeniza_sentencas(texto) for texto in textos_limpos]\n",
        "\n",
        "for lista in textos_tokenizados_sentencas:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI2bFFs3dLRJ",
        "outputId": "3b14fe93-578a-4b7c-e012-8fb2806ea47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['este e um exemplo de texto com html e caracteres especiais']\n",
            "['o gato preto correu rapidamente e pulou a cerca']\n",
            "['a menina estava feliz cantando e dancando no jardim']\n",
            "['o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo']\n",
            "['a tecnologia esta avancando a cada dia e transformando nossas vidas']\n",
            "['existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades']\n",
            "['e importante estudar com dedicacao para atingir nossos objetivos']\n",
            "['o processo de aprendizado de pln pode ser desafiador porem gratificante']\n",
            "['as redes sociais sao uma ferramenta poderosa para comunicacao e informacao']\n",
            "['eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo Stop Words\n",
        "\n",
        "textos_sem_stop_words = []\n",
        "for tokens_list in textos_tokenizados_palavra: # Iterate through each list of tokens\n",
        "  tokens_sem_sw = remove_stop_words(tokens_list) # Call the function for each list\n",
        "  textos_sem_stop_words.append(tokens_sem_sw) # Append the result to the main list\n",
        "\n",
        "for lista in textos_sem_stop_words:\n",
        "  print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsssRYJVdSZ2",
        "outputId": "7d27940b-f5d5-4fab-fdc5-b0d9bf3c9e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracteres', 'especiais']\n",
            "['gato', 'preto', 'correu', 'rapidamente', 'pulou', 'cerca']\n",
            "['menina', 'feliz', 'cantando', 'dancando', 'jardim']\n",
            "['filme', 'incrivel', 'achei', 'pouco', 'longo', 'sera', 'vale', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancando', 'cada', 'dia', 'transformando', 'vidas']\n",
            "['existem', 'muitos', 'desafios', 'mundo', 'ciencia', 'dados', 'tambem', 'muitas', 'oportunidades']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivos']\n",
            "['processo', 'aprendizado', 'pln', 'pode', 'desafiador', 'porem', 'gratificante']\n",
            "['redes', 'sociais', 'sao', 'ferramenta', 'poderosa', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanha', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando stemming aos tokens\n",
        "\n",
        "stemmed_textos = []\n",
        "for tokens_list in textos_sem_stop_words:\n",
        "    tokens_stemmed = aplica_stemming(tokens_list)\n",
        "    stemmed_textos.append(tokens_stemmed)\n",
        "\n",
        "for texto in stemmed_textos:\n",
        "  print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijGdmE3LF_bg",
        "outputId": "8181951d-437b-48ad-e871-c7d06431579f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exempl', 'text', 'html', 'caract', 'espec']\n",
            "['gat', 'pret', 'corr', 'rapid', 'pul', 'cerc']\n",
            "['menin', 'feliz', 'cant', 'danc', 'jardim']\n",
            "['film', 'incri', 'ach', 'pouc', 'long', 'ser', 'val', 'pen', 'assist', 'nov']\n",
            "['tecnolog', 'avanc', 'cad', 'dia', 'transform', 'vid']\n",
            "['exist', 'muit', 'desafi', 'mund', 'cienc', 'dad', 'tamb', 'muit', 'oportun']\n",
            "['import', 'estud', 'dedicaca', 'ating', 'obje']\n",
            "['process', 'aprend', 'pln', 'pod', 'desafi', 'por', 'gratific']\n",
            "['red', 'soc', 'sao', 'ferrament', 'poder', 'comunicaca', 'informaca']\n",
            "['ador', 'pizz', 'pepperon', 'tamb', 'amo', 'lasanh', 'bolonh']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicando lematização aos tokens\n",
        "\n",
        "lemm_textos = []\n",
        "for tokens_list in textos_sem_stop_words:\n",
        "    tokens_lematizados = aplica_lematizacao(tokens_list)\n",
        "    lemm_textos.append(tokens_lematizados)\n",
        "\n",
        "for texto in lemm_textos:\n",
        "  print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ztq1WB8wHIEm",
        "outputId": "f8b2adbb-a4ef-4b55-9b0c-8a334c4affad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['exemplo', 'texto', 'html', 'caracter', 'especial']\n",
            "['gato', 'preto', 'correr', 'rapidamente', 'pular', 'cerca']\n",
            "['menina', 'feliz', 'cantar', 'dancar', 'Jardim']\n",
            "['filme', 'incrivel', 'achar', 'pouco', 'longo', 'sera', 'valer', 'pena', 'assistir', 'novo']\n",
            "['tecnologia', 'avancar', 'cada', 'dia', 'transformar', 'vida']\n",
            "['existir', 'muito', 'desafio', 'mundo', 'ciencia', 'dar', 'tambem', 'muito', 'oportunidade']\n",
            "['importante', 'estudar', 'dedicacao', 'atingir', 'objetivo']\n",
            "['processo', 'aprendizar', 'pln', 'poder', 'desafiador', 'por', 'gratificante']\n",
            "['rede', 'social', 'sao', 'ferramentar', 'poderoso', 'comunicacao', 'informacao']\n",
            "['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanhar', 'bolonhesa']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analisando o impacto de cada etapa do Pré-Processamento\n",
        "Para concluir o projeto, vamos rever cada etapa de cada frase separadamente, para podermos analisar o impacto de cada ação durante o pré-processamento de textos, o que irá abrir uma gama de possibilidades para projetos futuros."
      ],
      "metadata": {
        "id": "YZtDHQtAzpeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for texto in range(len(textos)):\n",
        "  print(f\"Texto Original: {textos[texto]}\")\n",
        "  print(f\"Texto Limpo: {textos_limpos[texto]}\")\n",
        "  print(f\"Tokens de Palavras: {textos_tokenizados_palavra[texto]}\")\n",
        "  print(f\"Tokens de Sentenças: {textos_tokenizados_sentencas[texto]}\")\n",
        "  print(f\"Tokens sem Stop Words: {textos_sem_stop_words[texto]}\")\n",
        "  print(f\"Tokens Stemmed: {stemmed_textos[texto]}\")\n",
        "  print(f\"Tokens Lematizados: {lemm_textos[texto]}\")\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkoOYNwVzve9",
        "outputId": "2dc23dd5-774d-429d-b85d-8871504893c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto Original: <p>Este é um exemplo de texto <strong>com HTML</strong> e caracteres especiais! https://www.exemplo.com</p>\n",
            "Texto Limpo: este e um exemplo de texto com html e caracteres especiais\n",
            "Tokens de Palavras: ['este', 'e', 'um', 'exemplo', 'de', 'texto', 'com', 'html', 'e', 'caracteres', 'especiais']\n",
            "Tokens de Sentenças: ['este e um exemplo de texto com html e caracteres especiais']\n",
            "Tokens sem Stop Words: ['exemplo', 'texto', 'html', 'caracteres', 'especiais']\n",
            "Tokens Stemmed: ['exempl', 'text', 'html', 'caract', 'espec']\n",
            "Tokens Lematizados: ['exemplo', 'texto', 'html', 'caracter', 'especial']\n",
            "\n",
            "\n",
            "Texto Original: O GATO preto CORREU rapidamente e pulou a cerca.\n",
            "Texto Limpo: o gato preto correu rapidamente e pulou a cerca\n",
            "Tokens de Palavras: ['o', 'gato', 'preto', 'correu', 'rapidamente', 'e', 'pulou', 'a', 'cerca']\n",
            "Tokens de Sentenças: ['o gato preto correu rapidamente e pulou a cerca']\n",
            "Tokens sem Stop Words: ['gato', 'preto', 'correu', 'rapidamente', 'pulou', 'cerca']\n",
            "Tokens Stemmed: ['gat', 'pret', 'corr', 'rapid', 'pul', 'cerc']\n",
            "Tokens Lematizados: ['gato', 'preto', 'correr', 'rapidamente', 'pular', 'cerca']\n",
            "\n",
            "\n",
            "Texto Original: A menina estava feliz, cantando e dançando no jardim.\n",
            "Texto Limpo: a menina estava feliz cantando e dancando no jardim\n",
            "Tokens de Palavras: ['a', 'menina', 'estava', 'feliz', 'cantando', 'e', 'dancando', 'no', 'jardim']\n",
            "Tokens de Sentenças: ['a menina estava feliz cantando e dancando no jardim']\n",
            "Tokens sem Stop Words: ['menina', 'feliz', 'cantando', 'dancando', 'jardim']\n",
            "Tokens Stemmed: ['menin', 'feliz', 'cant', 'danc', 'jardim']\n",
            "Tokens Lematizados: ['menina', 'feliz', 'cantar', 'dancar', 'Jardim']\n",
            "\n",
            "\n",
            "Texto Original: O filme foi incrível, mas achei um pouco longo. Será que vale a pena assistir de novo?\n",
            "Texto Limpo: o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo\n",
            "Tokens de Palavras: ['o', 'filme', 'foi', 'incrivel', 'mas', 'achei', 'um', 'pouco', 'longo', 'sera', 'que', 'vale', 'a', 'pena', 'assistir', 'de', 'novo']\n",
            "Tokens de Sentenças: ['o filme foi incrivel mas achei um pouco longo sera que vale a pena assistir de novo']\n",
            "Tokens sem Stop Words: ['filme', 'incrivel', 'achei', 'pouco', 'longo', 'sera', 'vale', 'pena', 'assistir', 'novo']\n",
            "Tokens Stemmed: ['film', 'incri', 'ach', 'pouc', 'long', 'ser', 'val', 'pen', 'assist', 'nov']\n",
            "Tokens Lematizados: ['filme', 'incrivel', 'achar', 'pouco', 'longo', 'sera', 'valer', 'pena', 'assistir', 'novo']\n",
            "\n",
            "\n",
            "Texto Original: A tecnologia está avançando a cada dia e transformando nossas vidas.\n",
            "Texto Limpo: a tecnologia esta avancando a cada dia e transformando nossas vidas\n",
            "Tokens de Palavras: ['a', 'tecnologia', 'esta', 'avancando', 'a', 'cada', 'dia', 'e', 'transformando', 'nossas', 'vidas']\n",
            "Tokens de Sentenças: ['a tecnologia esta avancando a cada dia e transformando nossas vidas']\n",
            "Tokens sem Stop Words: ['tecnologia', 'avancando', 'cada', 'dia', 'transformando', 'vidas']\n",
            "Tokens Stemmed: ['tecnolog', 'avanc', 'cad', 'dia', 'transform', 'vid']\n",
            "Tokens Lematizados: ['tecnologia', 'avancar', 'cada', 'dia', 'transformar', 'vida']\n",
            "\n",
            "\n",
            "Texto Original: Existem muitos desafios no mundo da ciência de dados, mas também muitas oportunidades.\n",
            "Texto Limpo: existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades\n",
            "Tokens de Palavras: ['existem', 'muitos', 'desafios', 'no', 'mundo', 'da', 'ciencia', 'de', 'dados', 'mas', 'tambem', 'muitas', 'oportunidades']\n",
            "Tokens de Sentenças: ['existem muitos desafios no mundo da ciencia de dados mas tambem muitas oportunidades']\n",
            "Tokens sem Stop Words: ['existem', 'muitos', 'desafios', 'mundo', 'ciencia', 'dados', 'tambem', 'muitas', 'oportunidades']\n",
            "Tokens Stemmed: ['exist', 'muit', 'desafi', 'mund', 'cienc', 'dad', 'tamb', 'muit', 'oportun']\n",
            "Tokens Lematizados: ['existir', 'muito', 'desafio', 'mundo', 'ciencia', 'dar', 'tambem', 'muito', 'oportunidade']\n",
            "\n",
            "\n",
            "Texto Original: É importante estudar com dedicação para atingir nossos objetivos.\n",
            "Texto Limpo: e importante estudar com dedicacao para atingir nossos objetivos\n",
            "Tokens de Palavras: ['e', 'importante', 'estudar', 'com', 'dedicacao', 'para', 'atingir', 'nossos', 'objetivos']\n",
            "Tokens de Sentenças: ['e importante estudar com dedicacao para atingir nossos objetivos']\n",
            "Tokens sem Stop Words: ['importante', 'estudar', 'dedicacao', 'atingir', 'objetivos']\n",
            "Tokens Stemmed: ['import', 'estud', 'dedicaca', 'ating', 'obje']\n",
            "Tokens Lematizados: ['importante', 'estudar', 'dedicacao', 'atingir', 'objetivo']\n",
            "\n",
            "\n",
            "Texto Original: O processo de aprendizado de PLN pode ser desafiador, porém gratificante.\n",
            "Texto Limpo: o processo de aprendizado de pln pode ser desafiador porem gratificante\n",
            "Tokens de Palavras: ['o', 'processo', 'de', 'aprendizado', 'de', 'pln', 'pode', 'ser', 'desafiador', 'porem', 'gratificante']\n",
            "Tokens de Sentenças: ['o processo de aprendizado de pln pode ser desafiador porem gratificante']\n",
            "Tokens sem Stop Words: ['processo', 'aprendizado', 'pln', 'pode', 'desafiador', 'porem', 'gratificante']\n",
            "Tokens Stemmed: ['process', 'aprend', 'pln', 'pod', 'desafi', 'por', 'gratific']\n",
            "Tokens Lematizados: ['processo', 'aprendizar', 'pln', 'poder', 'desafiador', 'por', 'gratificante']\n",
            "\n",
            "\n",
            "Texto Original: As redes sociais são uma ferramenta poderosa para comunicação e informação.\n",
            "Texto Limpo: as redes sociais sao uma ferramenta poderosa para comunicacao e informacao\n",
            "Tokens de Palavras: ['as', 'redes', 'sociais', 'sao', 'uma', 'ferramenta', 'poderosa', 'para', 'comunicacao', 'e', 'informacao']\n",
            "Tokens de Sentenças: ['as redes sociais sao uma ferramenta poderosa para comunicacao e informacao']\n",
            "Tokens sem Stop Words: ['redes', 'sociais', 'sao', 'ferramenta', 'poderosa', 'comunicacao', 'informacao']\n",
            "Tokens Stemmed: ['red', 'soc', 'sao', 'ferrament', 'poder', 'comunicaca', 'informaca']\n",
            "Tokens Lematizados: ['rede', 'social', 'sao', 'ferramentar', 'poderoso', 'comunicacao', 'informacao']\n",
            "\n",
            "\n",
            "Texto Original: Eu adoro pizza de pepperoni, mas também amo lasanha à bolonhesa.\n",
            "Texto Limpo: eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa\n",
            "Tokens de Palavras: ['eu', 'adoro', 'pizza', 'de', 'pepperoni', 'mas', 'tambem', 'amo', 'lasanha', 'a', 'bolonhesa']\n",
            "Tokens de Sentenças: ['eu adoro pizza de pepperoni mas tambem amo lasanha a bolonhesa']\n",
            "Tokens sem Stop Words: ['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanha', 'bolonhesa']\n",
            "Tokens Stemmed: ['ador', 'pizz', 'pepperon', 'tamb', 'amo', 'lasanh', 'bolonh']\n",
            "Tokens Lematizados: ['adoro', 'pizza', 'pepperoni', 'tambem', 'amo', 'lasanhar', 'bolonhesa']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformando texto em números"
      ],
      "metadata": {
        "id": "sUd9-HXs1bOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementando Bag of Words - BoW\n",
        "\n",
        "\n",
        "*   `CountVectorizer` cria um objeto que pode fazer a contagem das palavras e criar o vocabulário.\n",
        "\n",
        "*   `vectorizer.fit(textos)` aprende o vocabulário do corpus de texto.\n",
        "\n",
        "*   `vectorizer.transform(textos)` transforma os textos em vetores, usando a contagem das palavras e o vocabulário que foi criado.\n",
        "\n",
        "*   `toarray()` transforma a matriz esparsa para uma matriz que pode ser visualizada facilmente.\n",
        "\n"
      ],
      "metadata": {
        "id": "D_SdEjCI12eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizando o Bow com CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "textos = [\n",
        "    \"o gato preto correu\",\n",
        "    \"o cão marrom correu\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(textos)\n",
        "bow_vectors = vectorizer.transform(textos).toarray()\n",
        "\n",
        "print(\"Vocabulário:\\n\", vectorizer.get_feature_names_out())\n",
        "print(f\"\\nVetor BoW:\\n\", bow_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIBJr8Xw1alp",
        "outputId": "cd9fea04-225d-40f7-b9d9-06cc66ffdc90"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulário:\n",
            " ['correu' 'cão' 'gato' 'marrom' 'preto']\n",
            "\n",
            "Vetor BoW:\n",
            " [[1 0 1 0 1]\n",
            " [1 1 0 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vantagens**\n",
        "\n",
        "  - Simples e fácil de entender.\n",
        "\n",
        "  - Fácil de implementar.\n",
        "\n",
        "**Desvantagens**\n",
        "\n",
        "- Perde a ordem das palavras e a estrutura da frase.\n",
        "\n",
        "- Não captura informações semânticas (ex: \"bom\" e \"ótimo\" são tratados como palavras totalmente diferentes, mesmo tendo significados semelhantes).\n",
        "\n",
        "- Considera todas as palavras igualmente importantes (ex: \"o\" e \"correu\" têm o mesmo peso, mas \"correu\" geralmente tem mais importância em uma frase).\n",
        "\n",
        "- Pode gerar vetores muito grandes com muitos zeros (esparsos) em corpora grandes."
      ],
      "metadata": {
        "id": "X782XE0b3VnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementando Term Frequency-Inverse Document Frequency | TF-IDF"
      ],
      "metadata": {
        "id": "4qc-A1tW3sGb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Documento 1:** \"o gato preto correu\"\n",
        "\n",
        "**Documento 2:** \"o cão marrom correu\"\n",
        "\n",
        "**Vocabulário:** O vocabulário continua o mesmo: `[\"o\", \"gato\", \"preto\", \"correu\", \"cão\", \"marrom\"]`\n",
        "\n",
        "1. TF (Term Frequency): Quantas vezes uma palavra aparece em um documento\n",
        "  - `TF(t, d) = (Número de vezes que o termo t aparece no documento d) / (Número total de termos no documento d)`.\n",
        "    - TF(o, doc1) = 1/4, TF(gato, doc1) = 1/4, TF(preto, doc1) = 1/4, TF(correu, doc1) = 1/4.\n",
        "\n",
        "    - TF(o, doc2) = 1/4, TF(cão, doc2) = 1/4, TF(marrom, doc2) = 1/4, TF(correu, doc2) = 1/4.\n",
        "\n",
        "2. IDF (Inverse Document Frequency): Quão rara é a palavra em toda a coleção de documentos.\n",
        "  - Calculado como `IDF(t) = log(N / df(t))`, onde `N` é o número total de documentos, e `df(t)` é o número de documentos que contêm o termo t.\n",
        "    - N = 2 (2 documentos)\n",
        "\n",
        "    - df(o) = 2 (aparece nos 2 documentos)\n",
        "\n",
        "    - df(gato) = 1 (aparece no documento 1)\n",
        "\n",
        "    - df(preto) = 1 (aparece no documento 1)\n",
        "\n",
        "    - df(correu) = 2 (aparece nos 2 documentos)\n",
        "\n",
        "    - df(cão) = 1 (aparece no documento 2)\n",
        "\n",
        "    - df(marrom) = 1 (aparece no documento 2)\n",
        "\n",
        "    - IDF(o) = log(2 / 2) = log(1) = 0\n",
        "\n",
        "    - IDF(gato) = log(2 / 1) = log(2) ≈ 0.301\n",
        "\n",
        "    - IDF(preto) = log(2 / 1) = log(2) ≈ 0.301\n",
        "\n",
        "    - IDF(correu) = log(2 / 2) = log(1) = 0\n",
        "\n",
        "    - DF(cão) = log(2 / 1) = log(2) ≈ 0.301\n",
        "\n",
        "    - IDF(marrom) = log(2 / 1) = log(2) ≈ 0.301\n",
        "\n",
        "    Observe que as palavras que aparecem em todos os documentos terão IDF = 0\n",
        "\n",
        "\n",
        "3. TF-IDF\n",
        "  - `TF-IDF(t, d) = TF(t, d) * IDF(t)`\n",
        "    - TF-IDF(o, doc1) = 1/4 * 0 = 0\n",
        "\n",
        "    - TF-IDF(gato, doc1) = 1/4 * 0.301 ≈ 0.075\n",
        "\n",
        "    - TF-IDF(preto, doc1) = 1/4 * 0.301 ≈ 0.075\n",
        "\n",
        "    - TF-IDF(correu, doc1) = 1/4 * 0 = 0\n",
        "\n",
        "    - TF-IDF(cão, doc1) = 1/4 * 0.301 = 0\n",
        "\n",
        "    - TF-IDF(marrom, doc1) = 1/4 * 0.301 = 0\n",
        "\n",
        "    - TF-IDF(o, doc2) = 1/4 * 0 = 0\n",
        "\n",
        "    - TF-IDF(gato, doc2) = 1/4 * 0.301 = 0\n",
        "\n",
        "    - TF-IDF(preto, doc2) = 1/4 * 0.301 = 0\n",
        "\n",
        "    - TF-IDF(correu, doc2) = 1/4 * 0 = 0\n",
        "\n",
        "    - TF-IDF(cão, doc2) = 1/4 * 0.301 ≈ 0.075\n",
        "\n",
        "    - TF-IDF(marrom, doc2) = 1/4 * 0.301 ≈ 0.075\n",
        "\n",
        "    Os vetores TF-IDF para os dois documentos seriam:\n",
        "\n",
        "    - Documento 1 = [0, 0.075, 0.075, 0, 0, 0]\n",
        "\n",
        "    - Documento 2 = [0, 0, 0, 0, 0.075, 0.075]"
      ],
      "metadata": {
        "id": "W-7iTgd04rGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizando TF-IDF com TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "textos = [\n",
        "    \"o gato preto correu\",\n",
        "    \"o cão marrom correu\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(textos)\n",
        "tfidf_vectors = vectorizer.transform(textos).toarray()\n",
        "\n",
        "print(\"Vocabulário:\\n\", vectorizer.get_feature_names_out())\n",
        "print(f\"\\nVetor TF-IDF:\\n\", tfidf_vectors)"
      ],
      "metadata": {
        "id": "d2gkebeq2myA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ff7c66-db7b-4307-cf3a-65f9a0ffde2c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulário:\n",
            " ['correu' 'cão' 'gato' 'marrom' 'preto']\n",
            "\n",
            "Vetor TF-IDF:\n",
            " [[0.44943642 0.         0.6316672  0.         0.6316672 ]\n",
            " [0.44943642 0.6316672  0.         0.6316672  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vantagens**\n",
        "\n",
        "- Dá mais peso às palavras que são importantes para um documento específico, reduzindo o peso de palavras comuns como \"o\".\n",
        "\n",
        "- Ajuda a identificar termos que são distintivos em um documento dentro de uma coleção.\n",
        "\n",
        "**Desvantagens**\n",
        "\n",
        "- Ainda perde a ordem das palavras.\n",
        "\n",
        "- Não captura informações semânticas."
      ],
      "metadata": {
        "id": "80D33SygCt-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escolha entre BoW e TF-IDF:\n",
        "\n",
        "**BoW:**\n",
        "\n",
        "Adequado para tarefas simples onde apenas a frequência das palavras é importante.\n",
        "\n",
        "Útil quando o vocabulário é relativamente pequeno.\n",
        "\n",
        "Mais fácil de implementar.\n",
        "\n",
        "**TF-IDF:**\n",
        "\n",
        "Adequado para tarefas onde a importância relativa das palavras é crucial, como classificação e recuperação de informação.\n",
        "\n",
        "Melhor para lidar com palavras comuns que não adicionam muito significado ao texto.\n",
        "\n",
        "Mais sofisticado e geralmente oferece melhor desempenho em tarefas mais complexas."
      ],
      "metadata": {
        "id": "1Nl3DPx4Dd6d"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMGkSQChCz_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}